# -*- coding: utf-8 -*-
"""Copie de PremierColabChefd'oeuvre_ArticleSeparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VXY5SnXixMusavJkj6dHJDMHA8Mxq0AA
"""

!wget https://huggingface.co/datasets/Jgmorenof/teaching_tools_2025/resolve/main/chef-douvre.zip

!unzip chef-douvre.zip

!PYTHONPATH=/content/chef-douvre/citlab-article-separation-new:$PYTHONPATH python3 /content/chef-douvre/citlab-article-separation-new/article_separation/image_segmentation/net_post_processing/run_net_post_processing.py \
    --path_to_image_list "/content/chef-douvre/page.lst" \
    --path_to_pb "/content/chef-douvre/citlab-article-separation-new/nets/separator_detection_net.pb" \
    --mode "heading" \
    --num_processes 4

"""
reconstruct_pagexml_sbert.py
Pipeline SBERT pour PAGE-XML (Transkribus / PAGE).
- Lit les TextRegion/TextLine + Coords points
- Extrait TextEquiv/Unicode (ou fallback itertext)
- Normalise les bbox par rapport à imageWidth/imageHeight
- Fusionne petits blocs proches
- Calcule embeddings SBERT en batch
- Clustering (HDBSCAN via UMAP ou Agglomerative)
- Sauvegarde JSON + textes par cluster

Usage:
    pip install sentence-transformers lxml numpy scikit-learn hdbscan umap-learn tqdm
    python reconstruct_pagexml_sbert.py --xml_folder path/to/xmls --out out_folder
"""

import os
import json
import argparse
from pathlib import Path
from lxml import etree
from sentence_transformers import SentenceTransformer
import numpy as np
from tqdm import tqdm
from sklearn.cluster import AgglomerativeClustering
from collections import defaultdict

# Optional libs
try:
    import umap
    import hdbscan
    HAS_HDBSCAN = True
except Exception:
    HAS_HDBSCAN = False

# ----------------- Parser adapté PAGE-XML -----------------
PAGE_NS = "http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15"
NSMAP = {"ns": PAGE_NS}

def parse_page_xml(path):
    """
    Parse PAGE XML file and return list of blocks:
    {text, xmin, ymin, xmax, ymax, imageWidth, imageHeight, source}
    Coordinates are normalized to [0,1] by imageWidth/imageHeight.
    """
    tree = etree.parse(str(path))
    root = tree.getroot()

    # page dims (fallback to Page attributes)
    page_elem = root.find(".//ns:Page", namespaces=NSMAP)
    if page_elem is not None:
        w = page_elem.get("imageWidth")
        h = page_elem.get("imageHeight")
        try:
            image_w = float(w) if w else None
            image_h = float(h) if h else None
        except:
            image_w = image_h = None
    else:
        image_w = image_h = None

    blocks = []

    # parcourir TextRegion
    for region in root.findall(".//ns:TextRegion", namespaces=NSMAP):
        # region-level coords (as fallback)
        coords_node = region.find(".//ns:Coords", namespaces=NSMAP)
        if coords_node is not None and coords_node.get("points"):
            xmin_r, ymin_r, xmax_r, ymax_r = coords_from_points(coords_node.get("points"))
        else:
            xmin_r = ymin_r = xmax_r = ymax_r = None

        # privilégier TextLine elements
        textlines = region.findall(".//ns:TextLine", namespaces=NSMAP)
        if not textlines:
            # fallback: take region text equiv
            text_equiv = region.find(".//ns:TextEquiv/ns:Unicode", namespaces=NSMAP)
            text = text_equiv.text.strip() if text_equiv is not None and text_equiv.text else None
            if text:
                # bbox fallback: region coords or 0..1
                xmin, ymin, xmax, ymax = (xmin_r, ymin_r, xmax_r, ymax_r)
                if None in (xmin,xmax,ymin,ymax):
                    xmin=ymin=0.0; xmax=1.0; ymax=1.0
                blocks.append({
                    "text": text,
                    "xmin": xmin, "ymin": ymin, "xmax": xmax, "ymax": ymax,
                    "image_w": image_w, "image_h": image_h, "source": str(path)
                })

            continue

        for tl in textlines:
            # texte (TextEquiv/Unicode si présent)
            te = tl.find(".//ns:TextEquiv/ns:Unicode", namespaces=NSMAP)
            if te is not None and te.text and te.text.strip():
                text = te.text.strip()
            else:
                # fallback to concatenated chars
                text = "".join(tl.itertext()).strip()

            if not text:
                continue

            # coords from TextLine Coords if present else fallback to region coords
            coords_node = tl.find(".//ns:Coords", namespaces=NSMAP)
            if coords_node is not None and coords_node.get("points"):
                xmin, ymin, xmax, ymax = coords_from_points(coords_node.get("points"))
            elif coords_node is None and xmin_r is not None:
                xmin, ymin, xmax, ymax = xmin_r, ymin_r, xmax_r, ymax_r
            else:
                # final fallback
                xmin = ymin = 0.0
                xmax = image_w if image_w else 1.0
                ymax = image_h if image_h else 1.0

            # Normalize if image dims known
            if image_w and image_h:
                # convert absolute to [0,1]
                nxmin = xmin / image_w
                nxmax = xmax / image_w
                nymin = ymin / image_h
                nymax = ymax / image_h
            else:
                # If coords already likely small (e.g. normalized), clamp to [0,1]
                # Otherwise we will normalize later globally.
                nxmin, nymin, nxmax, nymax = xmin, ymin, xmax, ymax

            blocks.append({
                "text": text,
                "xmin": float(nxmin), "ymin": float(nymin),
                "xmax": float(nxmax), "ymax": float(nymax),
                "image_w": image_w, "image_h": image_h,
                "source": str(path)
            })
            # Print source here
            print(f"Source: {str(path)}")
    return blocks

def coords_from_points(points_str):
    """
    points_str: "x1,y1 x2,y2 x3,y3 ..."
    returns xmin,ymin,xmax,ymax (floats)
    """
    pts = []
    for pair in points_str.strip().split():
        try:
            x_s,y_s = pair.split(",")
            pts.append((float(x_s), float(y_s)))
        except:
            continue
    xs = [p[0] for p in pts] if pts else [0.0]
    ys = [p[1] for p in pts] if pts else [0.0]
    return min(xs), min(ys), max(xs), max(ys)

# ----------------- Utilities -----------------
def load_all_blocks(xml_folder):
    xml_folder = Path(xml_folder)
    all_blocks = []
    for f in xml_folder.glob("*.xml"):
        try:
            bls = parse_page_xml(f)
            all_blocks.extend(bls)
        except Exception as e:
            print(f"Erreur parsing {f}: {e}")
    # If many files had absolute coords without normalization,
    # we can normalize by each page's image_w/image_h already done.
    # As fallback, if any coords >1 and image dims unknown, we'll min-max normalize per file later if needed.
    return all_blocks

def merge_short_blocks(blocks, char_threshold=40, y_tol=0.03, x_tol=0.15):
    """
    Blocks coords are normalized [0,1].
    Merge short blocks with a vertical neighbor in the same column.
    y_tol, x_tol are normalized tolerances.
    """
    blocks_by_source = defaultdict(list)
    for b in blocks:
        blocks_by_source[b["source"]].append(b)

    merged = []
    for src, bls in blocks_by_source.items():
        bls_sorted = sorted(bls, key=lambda b: (b["xmin"], b["ymin"]))
        used = [False]*len(bls_sorted)
        for i, b in enumerate(bls_sorted):
            if used[i]:
                continue
            text = b["text"]
            if len(text) < char_threshold:
                best_j = None
                for j in range(len(bls_sorted)):
                    if i == j or used[j]:
                        continue
                    bj = bls_sorted[j]
                    # same column if x overlap or close horizontally
                    x_overlap = not (bj["xmax"] < b["xmin"] or bj["xmin"] > b["xmax"])
                    same_col = x_overlap or abs(bj["xmin"] - b["xmin"]) < x_tol
                    vertically_close = abs((bj["ymin"]+bj["ymax"])/2 - (b["ymin"]+b["ymax"])/2) < y_tol
                    if same_col and vertically_close:
                        best_j = j
                        break
                if best_j is not None:
                    bj = bls_sorted[best_j]
                    # append b text before or after depending on y positions (higher y is lower on page)
                    if b["ymin"] < bj["ymin"]:
                        new_text = b["text"] + " " + bj["text"]
                    else:
                        new_text = bj["text"] + " " + b["text"]
                    bj["text"] = new_text
                    bj["xmin"] = min(bj["xmin"], b["xmin"])
                    bj["ymin"] = min(bj["ymin"], b["ymin"])
                    bj["xmax"] = max(bj["xmax"], b["xmax"])
                    bj["ymax"] = max(bj["ymax"], b["ymax"])
                    used[i] = True
                else:
                    merged.append(b); used[i] = True
            else:
                merged.append(b); used[i] = True
    return merged

# ----------------- SBERT embeddings -----------------
def compute_sbert_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=64, device=None):
    model = SentenceTransformer(model_name, device=device)
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)
    return embeddings, model

# ----------------- Clustering -----------------
def cluster_embeddings_hdbscan(embeddings, min_cluster_size=3, min_samples=1, umap_n_neighbors=15, umap_min_dist=0.0, umap_n_components=32):
    if not HAS_HDBSCAN:
        raise RuntimeError("HDBSCAN/UMAP non installés. Installez 'umap-learn' et 'hdbscan' ou utilisez 'agglomerative'.")
    reducer = umap.UMAP(n_neighbors=umap_n_neighbors, min_dist=umap_min_dist, n_components=umap_n_components, random_state=42)
    emb_reduced = reducer.fit_transform(embeddings)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean')
    labels = clusterer.fit_predict(emb_reduced)
    return labels, emb_reduced, clusterer

def cluster_embeddings_agglomerative(embeddings, n_clusters=50):
    clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='cosine', linkage='average')
    labels = clustering.fit_predict(embeddings)
    return labels

# ----------------- Sort blocks spatially -----------------
def sort_blocks_in_cluster(blocks):
    # tri top->bottom (ymin), puis left->right (xmin)
    return sorted(blocks, key=lambda b: (b['ymin'], b['xmin']))

# ----------------- Pipeline for Sbert et hdbscan methode  -----------------
def pipeline(xml_folder, output_folder,
             sbert_model='all-MiniLM-L6-v2',
             merge_short=True,
             char_threshold=40,
             clustering_method='hdbscan',
             clustering_kwargs=None):
    os.makedirs(output_folder, exist_ok=True)
    if clustering_kwargs is None:
        clustering_kwargs = {}

    print("[1] Extraction blocs depuis PAGE-XML...")
    blocks = load_all_blocks(xml_folder)
    print(f"  => blocs extraits: {len(blocks)}")

    # ensure coords are in [0,1]
    # if any coord >1, and page image dims are unknown, we can min-max normalize per source
    for src, bls in defaultdict(list, {b['source']:[] for b in blocks}).items():
        pass
    # Quick fix: if any coord >1.5, do per-file normalization by max coords
    max_coord = max(max(b['xmax'], b['ymax'], b['xmin'], b['ymin']) for b in blocks) if blocks else 1.0
    if max_coord > 1.5:
        print("  -> Normalisation des coordonnées (valeurs absolues détectées)...")
        # normalize per-file using image_w/image_h if available
        for b in blocks:
            if b['image_w'] and b['image_h']:
                b['xmin'] /= b['image_w']; b['xmax'] /= b['image_w']
                b['ymin'] /= b['image_h']; b['ymax'] /= b['image_h']
            else:
                # global min-max
                b['xmin'] /= max_coord; b['xmax'] /= max_coord
                b['ymin'] /= max_coord; b['ymax'] /= max_coord

    if merge_short:
        print("[2] Fusion des petits blocs proches...")
        blocks = merge_short_blocks(blocks, char_threshold=char_threshold)
        print(f"  => blocs après fusion: {len(blocks)}")

    texts = [b['text'] for b in blocks]

    print("[3] Calcul embeddings SBERT...")
    embeddings, model = compute_sbert_embeddings(texts, model_name=sbert_model)

    print("[4] Clustering...")
    if clustering_method == 'hdban':
        labels, emb_reduced, clusterer = cluster_embeddings_hdbscan(embeddings, **clustering_kwargs)
    else:
        n_clusters = clustering_kwargs.get('n_clusters', 50)
        labels = cluster_embeddings_agglomerative(embeddings, n_clusters=n_clusters)
        emb_reduced = None
        clusterer = None

    # Regrouper
    articles = defaultdict(list)
    for label, b in zip(labels, blocks):
        articles[int(label)].append(b)

    # Sauvegarde
    results = []
    for label, bls in articles.items():
        sorted_blks = sort_blocks_in_cluster(bls)
        article_text = "\n".join([bb['text'] for bb in sorted_blks])
        results.append({
            "label": int(label),
            "n_blocks": len(sorted_blks),
            "text": article_text,
            "blocks": sorted_blks
        })
        # sauvegarde fichier texte
        cluster_name = f"article_{label}" if label != -1 else "noise"
        fname = os.path.join(output_folder, f"{cluster_name}.txt")
        with open(fname, "w", encoding="utf-8") as fo:
            fo.write(article_text)

    with open(os.path.join(output_folder, "reconstructed_articles.json"), "w", encoding="utf-8") as fo:
        json.dump(results, fo, ensure_ascii=False, indent=2)

    print("[DONE] Résultats dans:", output_folder)
    return results, labels, embeddings, emb_reduced, clusterer

# ----------------- Pipeline for Sbert et Aglomorative clusturing -----------------

def pipeline_new(xml_folder, output_folder,
             sbert_model='all-MiniLM-L6-v2',
             merge_short=True,
             char_threshold=40,
             clustering_method='agglomerative_clustering',   # default corrected
             clustering_kwargs=None):
    os.makedirs(output_folder, exist_ok=True)
    if clustering_kwargs is None:
        clustering_kwargs = {}

    print("[1] Extraction blocs depuis PAGE-XML...")
    blocks = load_all_blocks(xml_folder)
    print(f"  => blocs extraits: {len(blocks)}")

    # ensure coords are in [0,1]
    max_coord = max(max(b.get('xmax',0), b.get('ymax',0), b.get('xmin',0), b.get('ymin',0)) for b in blocks) if blocks else 1.0
    if max_coord > 1.5:
        print("  -> Normalisation des coordonnées (valeurs absolues détectées)...")
        for b in blocks:
            if b.get('image_w') and b.get('image_h'):
                b['xmin'] /= b['image_w']; b['xmax'] /= b['image_w']
                b['ymin'] /= b['image_h']; b['ymax'] /= b['image_h']
            else:
                b['xmin'] /= max_coord; b['xmax'] /= max_coord
                b['ymin'] /= max_coord; b['ymax'] /= max_coord

    if merge_short:
        print("[2] Fusion des petits blocs proches...")
        blocks = merge_short_blocks(blocks, char_threshold=char_threshold)
        print(f"  => blocs après fusion: {len(blocks)}")

    texts = [b.get('text', '') for b in blocks]

    print("[3] Calcul embeddings SBERT...")
    embeddings, model = compute_sbert_embeddings(texts, model_name=sbert_model)

    print("[4] Clustering...")
    method = (clustering_method or '').strip().lower()

    if method in {'hdbscan'}:
        # require hdbscan-related function implemented
        labels, emb_reduced, clusterer = cluster_embeddings_hdbscan(embeddings, **clustering_kwargs)
    elif method in {'agglomerative_clustering'}:
        # agglomerative_clustering: expect n_clusters in clustering_kwargs
        n_clusters = clustering_kwargs.get('n_clusters', 50)
        labels = cluster_embeddings_agglomerative(embeddings, n_clusters=n_clusters)
        emb_reduced = None
        clusterer = None
    else:
        raise ValueError(f"Unknown clustering_method '{clustering_method}'. Use 'hdbscan' or 'agglomerative'.")

    # Validate labels length
    if len(labels) != len(blocks):
        raise RuntimeError(f"Mismatch: len(labels)={len(labels)} but len(blocks)={len(blocks)}")

    # Regrouper
    articles = defaultdict(list)
    for label, b in zip(labels, blocks):
        articles[int(label)].append(b)

    # Sauvegarde
    results = []
    for label, bls in articles.items():
        sorted_blks = sort_blocks_in_cluster(bls)
        article_text = "\n".join([bb.get('text','') for bb in sorted_blks])
        results.append({
            "label": int(label),
            "n_blocks": len(sorted_blks),
            "text": article_text,
            "blocks": sorted_blks
        })
        cluster_name = f"article_{label}" if label != -1 else "noise"
        fname = os.path.join(output_folder, f"{cluster_name}.txt")
        with open(fname, "w", encoding="utf-8") as fo:
            fo.write(article_text)

    with open(os.path.join(output_folder, "reconstructed_articles.json"), "w", encoding="utf-8") as fo:
        json.dump(results, fo, ensure_ascii=False, indent=2)

    print("[DONE] Résultats dans:", output_folder)
    return results, labels, embeddings, emb_reduced, clusterer

# ----------------- CLI -----------------
# if __name__ == "__main__":
#     p = argparse.ArgumentParser()
#     p.add_argument("--xml_folder", required=True, help="Dossier contenant les PAGE-XML (.xml)")
#     p.add_argument("--out", required=True, help="Dossier de sortie")
#     p.add_argument("--model", default="all-MiniLM-L6-v2", help="SBERT model")
#     p.add_argument("--merge_short", action="store_true", help="Activer fusion des petits blocs")
#     p.add_argument("--char_threshold", type=int, default=40, help="Seuil chars pour fusion")
#     p.add_argument("--cluster_method", default="hdbscan", choices=["hdbscan","agglomerative"])
#     p.add_argument("--min_cluster_size", type=int, default=3)
#     p.add_argument("--min_samples", type=int, default=1)
#     p.add_argument("--umap_n_components", type=int, default=32)
#     args = p.parse_args()

#     cluster_kwargs = {"min_cluster_size": args.min_cluster_size, "min_samples": args.min_samples, "umap_n_components": args.umap_n_components}
#     pipeline(args.xml_folder, args.out, sbert_model=args.model, merge_short=args.merge_short,
#              char_threshold=args.char_threshold, clustering_method=args.cluster_method,
#              clustering_kwargs=cluster_kwargs)

# Call the pipeline function directly with appropriate arguments
xml_folder_path = "/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2" # Replace with your XML folder path
output_folder_path = "/content/chef-douvre/output_sbert" # Replace with your desired output folder path

#Sbert et hdbscan
#pipeline(xml_folder_path, output_folder_path)

#Sbert et agglomorative clustering
pipeline_new(xml_folder_path, output_folder_path)

# from google.colab import files

# si tu veux télécharger un dossier complet
# !zip -r resultats.zip /content/chef-douvre/output_sbert
# files.download("resultats.zip")
