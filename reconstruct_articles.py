# -*- coding: utf-8 -*-
"""llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_PVrLBtWTJEe4wgI2Vy9wXKCczkFQG3d
"""

!wget https://huggingface.co/datasets/Jgmorenof/teaching_tools_2025/resolve/main/chef-douvre.zip

!unzip chef-douvre.zip

import os
import glob

# 1. Spécifiez le chemin de votre dossier
# **⚠️ CHANGEZ CETTE LIGNE**
DOSSIER = '/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2'

# 2. Trouvez tous les fichiers JPG/JPEG (ignore la casse)
fichiers_jpg = glob.glob(os.path.join(DOSSIER, '*.[jJ][pP][gG]'))
fichiers_jpeg = glob.glob(os.path.join(DOSSIER, '*.[jJ][pP][eE][gG]'))

tous_les_fichiers = fichiers_jpg + fichiers_jpeg

# 3. Supprimez-les un par un
for fichier in tous_les_fichiers:
    try:
        os.remove(fichier)
        print(f"Supprimé : {fichier}")
    except OSError as e:
        print(f"Erreur lors de la suppression de {fichier} : {e}")

print(f"\nTerminé. {len(tous_les_fichiers)} fichier(s) ciblé(s).")

import os
import json
import argparse
from pathlib import Path
from lxml import etree
from sentence_transformers import SentenceTransformer
import numpy as np
from tqdm import tqdm
from sklearn.cluster import AgglomerativeClustering
from collections import defaultdict
from sklearn.metrics import adjusted_rand_score, homogeneity_score, completeness_score, v_measure_score

# Optional libs
try:
    import umap
    import hdbscan
    HAS_HDBSCAN = True
except Exception:
    HAS_HDBSCAN = False

# ----------------- Parser adapté PAGE-XML -----------------
PAGE_NS = "http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15"
NSMAP = {"ns": PAGE_NS}

def parse_page_xml(path):
    """
    Parse PAGE XML file and return list of blocks:
    {text, xmin, ymin, xmax, ymax, imageWidth, imageHeight, source}
    Coordinates are normalized to [0,1] by imageWidth/imageHeight.
    """
    tree = etree.parse(str(path))
    root = tree.getroot()

    # page dims (fallback to Page attributes)
    page_elem = root.find(".//ns:Page", namespaces=NSMAP)
    if page_elem is not None:
        w = page_elem.get("imageWidth")
        h = page_elem.get("imageHeight")
        try:
            image_w = float(w) if w else None
            image_h = float(h) if h else None
        except:
            image_w = image_h = None
    else:
        image_w = image_h = None

    blocks = []

    # parcourir TextRegion
    for region in root.findall(".//ns:TextRegion", namespaces=NSMAP):
        # region-level coords (as fallback)
        coords_node = region.find(".//ns:Coords", namespaces=NSMAP)
        if coords_node is not None and coords_node.get("points"):
            xmin_r, ymin_r, xmax_r, ymax_r = coords_from_points(coords_node.get("points"))
        else:
            xmin_r = ymin_r = xmax_r = ymax_r = None

        # privilégier TextLine elements
        textlines = region.findall(".//ns:TextLine", namespaces=NSMAP)
        if not textlines:
            # fallback: take region text equiv
            text_equiv = region.find(".//ns:TextEquiv/ns:Unicode", namespaces=NSMAP)
            text = text_equiv.text.strip() if text_equiv is not None and text_equiv.text else None
            if text:
                # bbox fallback: region coords or 0..1
                xmin, ymin, xmax, ymax = (xmin_r, ymin_r, xmax_r, ymax_r)
                if None in (xmin,xmax,ymin,ymax):
                    xmin=ymin=0.0; xmax=1.0; ymax=1.0
                blocks.append({
                    "text": text,
                    "xmin": xmin, "ymin": ymin, "xmax": xmax, "ymax": ymax,
                    "image_w": image_w, "image_h": image_h, "source": str(path)
                })

            continue

        for tl in textlines:
            # texte (TextEquiv/Unicode si présent)
            te = tl.find(".//ns:TextEquiv/ns:Unicode", namespaces=NSMAP)
            if te is not None and te.text and te.text.strip():
                text = te.text.strip()
            else:
                # fallback to concatenated chars
                text = "".join(tl.itertext()).strip()

            if not text:
                continue

            # coords from TextLine Coords if present else fallback to region coords
            coords_node = tl.find(".//ns:Coords", namespaces=NSMAP)
            if coords_node is not None and coords_node.get("points"):
                xmin, ymin, xmax, ymax = coords_from_points(coords_node.get("points"))
            elif coords_node is None and xmin_r is not None:
                xmin, ymin, xmax, ymax = xmin_r, ymin_r, xmax_r, ymax_r
            else:
                # final fallback
                xmin = ymin = 0.0
                xmax = image_w if image_w else 1.0
                ymax = image_h if image_h else 1.0

            # Normalize if image dims known
            if image_w and image_h:
                # convert absolute to [0,1]
                nxmin = xmin / image_w
                nxmax = xmax / image_w
                nymin = ymin / image_h
                nymax = ymax / image_h
            else:
                # If coords already likely small (e.g. normalized), clamp to [0,1]
                # Otherwise we will normalize later globally.
                nxmin, nymin, nxmax, nymax = xmin, ymin, xmax, ymax

            blocks.append({
                "text": text,
                "xmin": float(nxmin), "ymin": float(nymin),
                "xmax": float(nxmax), "ymax": float(nymax),
                "image_w": image_w, "image_h": image_h,
                "source": str(path)
            })
            # Print source here
            print(f"Source: {str(path)}")
    return blocks

def coords_from_points(points_str):
    """
    points_str: "x1,y1 x2,y2 x3,y3 ..."
    returns xmin,ymin,xmax,ymax (floats)
    """
    pts = []
    for pair in points_str.strip().split():
        try:
            x_s,y_s = pair.split(",")
            pts.append((float(x_s), float(y_s)))
        except:
            continue
    xs = [p[0] for p in pts] if pts else [0.0]
    ys = [p[1] for p in pts] if pts else [0.0]
    return min(xs), min(ys), max(xs), max(ys)

# ----------------- Utilities -----------------
def load_all_blocks(xml_folder):
    xml_folder = Path(xml_folder)
    all_blocks = []
    for f in xml_folder.glob("*.xml"):
        try:
            bls = parse_page_xml(f)
            all_blocks.extend(bls)
        except Exception as e:
            print(f"Erreur parsing {f}: {e}")
    # If many files had absolute coords without normalization,
    # we can normalize by each page's image_w/image_h already done.
    # As fallback, if any coords >1 and image dims unknown, we'll min-max normalize per file later if needed.
    return all_blocks

def merge_short_blocks(blocks, char_threshold=40, y_tol=0.03, x_tol=0.15):
    """
    Blocks coords are normalized [0,1].
    Merge short blocks with a vertical neighbor in the same column.
    y_tol, x_tol are normalized tolerances.
    """
    blocks_by_source = defaultdict(list)
    for b in blocks:
        blocks_by_source[b["source"]].append(b)

    merged = []
    for src, bls in blocks_by_source.items():
        bls_sorted = sorted(bls, key=lambda b: (b["xmin"], b["ymin"]))
        used = [False]*len(bls_sorted)
        for i, b in enumerate(bls_sorted):
            if used[i]:
                continue
            text = b["text"]
            if len(text) < char_threshold:
                best_j = None
                for j in range(len(bls_sorted)):
                    if i == j or used[j]:
                        continue
                    bj = bls_sorted[j]
                    # same column if x overlap or close horizontally
                    x_overlap = not (bj["xmax"] < b["xmin"] or bj["xmin"] > b["xmax"])
                    same_col = x_overlap or abs(bj["xmin"] - b["xmin"]) < x_tol
                    vertically_close = abs((bj["ymin"]+bj["ymax"])/2 - (b["ymin"]+b["ymax"])/2) < y_tol
                    if same_col and vertically_close:
                        best_j = j
                        break
                if best_j is not None:
                    bj = bls_sorted[best_j]
                    # append b text before or after depending on y positions (higher y is lower on page)
                    if b["ymin"] < bj["ymin"]:
                        new_text = b["text"] + " " + bj["text"]
                    else:
                        new_text = bj["text"] + " " + b["text"]
                    bj["text"] = new_text
                    bj["xmin"] = min(bj["xmin"], b["xmin"])
                    bj["ymin"] = min(bj["ymin"], b["ymin"])
                    bj["xmax"] = max(bj["xmax"], b["xmax"])
                    bj["ymax"] = max(bj["ymax"], b["ymax"])
                    used[i] = True
                else:
                    merged.append(b); used[i] = True
            else:
                merged.append(b); used[i] = True
    return merged

# ----------------- SBERT embeddings -----------------
def compute_sbert_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=64, device=None):
    model = SentenceTransformer(model_name, device=device)
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)
    return embeddings, model

# ----------------- Clustering -----------------
def cluster_embeddings_hdbscan(embeddings, min_cluster_size=3, min_samples=1, umap_n_neighbors=15, umap_min_dist=0.0, umap_n_components=32):
    if not HAS_HDBSCAN:
        raise RuntimeError("HDBSCAN/UMAP non installés. Installez 'umap-learn' et 'hdbscan' ou utilisez 'agglomerative'.")
    reducer = umap.UMAP(n_neighbors=umap_n_neighbors, min_dist=umap_min_dist, n_components=umap_n_components, random_state=42)
    emb_reduced = reducer.fit_transform(embeddings)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, metric='euclidean')
    labels = clusterer.fit_predict(emb_reduced)
    return labels, emb_reduced, clusterer

def cluster_embeddings_agglomerative(embeddings, n_clusters=50):
    clustering = AgglomerativeClustering(n_clusters=n_clusters, metric='cosine', linkage='average')
    labels = clustering.fit_predict(embeddings)
    return labels

# ----------------- Sort blocks spatially -----------------
def sort_blocks_in_cluster(blocks):
    # tri top->bottom (ymin), puis left->right (xmin)
    return sorted(blocks, key=lambda b: (b['ymin'], b['xmin']))

# ----------------- Pipeline for Sbert et hdbscan methode  -----------------
def pipeline(xml_folder, output_folder,
             sbert_model='all-MiniLM-L6-v2',
             merge_short=True,
             char_threshold=40,
             clustering_method='hdbscan',
             clustering_kwargs=None):
    os.makedirs(output_folder, exist_ok=True)
    if clustering_kwargs is None:
        clustering_kwargs = {}

    print("[1] Extraction blocs depuis PAGE-XML...")
    blocks = load_all_blocks(xml_folder)
    print(f"  => blocs extraits: {len(blocks)}")

    # ensure coords are in [0,1]
    # if any coord >1, and page image dims are unknown, we can min-max normalize per file later if needed.
    for src, bls in defaultdict(list, {b['source']:[] for b in blocks}).items():
        pass
    # Quick fix: if any coord >1.5, do per-file normalization by max coords
    max_coord = max(max(b['xmax'], b['ymax'], b['xmin'], b['ymin']) for b in blocks) if blocks else 1.0
    if max_coord > 1.5:
        print("  -> Normalisation des coordonnées (valeurs absolues détectées)...")
        # normalize per-file using image_w/image_h if available
        for b in blocks:
            if b['image_w'] and b['image_h']:
                b['xmin'] /= b['image_w']; b['xmax'] /= b['image_w']
                b['ymin'] /= b['image_h']; b['ymax'] /= b['image_h']
            else:
                # global min-max
                b['xmin'] /= max_coord; b['xmax'] /= max_coord
                b['ymin'] /= max_coord; b['ymax'] /= max_coord

    if merge_short:
        print("[2] Fusion des petits blocs proches...")
        blocks = merge_short_blocks(blocks, char_threshold=char_threshold)
        print(f"  => blocs après fusion: {len(blocks)}")

    texts = [b['text'] for b in blocks]

    print("[3] Calcul embeddings SBERT...")
    embeddings, model = compute_sbert_embeddings(texts, model_name=sbert_model)

    print("[4] Clustering...")
    if clustering_method == 'hdban':
        labels, emb_reduced, clusterer = cluster_embeddings_hdbscan(embeddings, **clustering_kwargs)
    else:
        n_clusters = clustering_kwargs.get('n_clusters', 50)
        labels = cluster_embeddings_agglomerative(embeddings, n_clusters=n_clusters)
        emb_reduced = None
        clusterer = None

    # Regrouper
    articles = defaultdict(list)
    for label, b in zip(labels, blocks):
        articles[int(label)].append(b)

    # Sauvegarde
    results = []
    for label, bls in articles.items():
        sorted_blks = sort_blocks_in_cluster(bls)
        article_text = "\n".join([bb['text'] for bb in sorted_blks])
        results.append({
            "label": int(label),
            "n_blocks": len(sorted_blks),
            "text": article_text,
            "blocks": sorted_blks
        })
        # sauvegarde fichier texte
        cluster_name = f"article_{label}" if label != -1 else "noise"
        fname = os.path.join(output_folder, f"{cluster_name}.txt")
        with open(fname, "w", encoding="utf-8") as fo:
            fo.write(article_text)

    with open(os.path.join(output_folder, "reconstructed_articles.json"), "w", encoding="utf-8") as fo:
        json.dump(results, fo, ensure_ascii=False, indent=2)

    print("[DONE] Résultats dans:", output_folder)
    return results, labels, embeddings, emb_reduced, clusterer

# ----------------- Pipeline for Sbert et Aglomorative clusturing -----------------

def pipeline_new(xml_folder, output_folder,
             sbert_model='all-MiniLM-L6-v2',
             merge_short=True,
             char_threshold=40,
             clustering_method='agglomerative_clustering',   # default corrected
             clustering_kwargs=None):
    os.makedirs(output_folder, exist_ok=True)
    if clustering_kwargs is None:
        clustering_kwargs = {}

    print("[1] Extraction blocs depuis PAGE-XML...")
    blocks = load_all_blocks(xml_folder)
    print(f"  => blocs extraits: {len(blocks)}")

    # ensure coords are in [0,1]
    max_coord = max(max(b.get('xmax',0), b.get('ymax',0), b.get('xmin',0), b.get('ymin',0)) for b in blocks) if blocks else 1.0
    if max_coord > 1.5:
        print("  -> Normalisation des coordonnées (valeurs absolues détectées)...")
        for b in blocks:
            if b.get('image_w') and b.get('image_h'):
                b['xmin'] /= b['image_w']; b['xmax'] /= b['image_w']
                b['ymin'] /= b['image_h']; b['ymax'] /= b['image_h']
            else:
                b['xmin'] /= max_coord; b['xmax'] /= max_coord
                b['ymin'] /= max_coord; b['ymax'] /= max_coord

    if merge_short:
        print("[2] Fusion des petits blocs proches...")
        blocks = merge_short_blocks(blocks, char_threshold=char_threshold)
        print(f"  => blocs après fusion: {len(blocks)}")

    texts = [b.get('text', '') for b in blocks]

    print("[3] Calcul embeddings SBERT...")
    embeddings, model = compute_sbert_embeddings(texts, model_name=sbert_model)

    print("[4] Clustering...")
    method = (clustering_method or '').strip().lower()

    if method in {'hdbscan'}:
        # require hdbscan-related function implemented
        labels, emb_reduced, clusterer = cluster_embeddings_hdbscan(embeddings, **clustering_kwargs)
    elif method in {'agglomerative_clustering'}:
        # agglomerative_clustering: expect n_clusters in clustering_kwargs
        n_clusters = clustering_kwargs.get('n_clusters', 50)
        labels = cluster_embeddings_agglomerative(embeddings, n_clusters=n_clusters)
        emb_reduced = None
        clusterer = None
    else:
        raise ValueError(f"Unknown clustering_method '{clustering_method}'. Use 'hdbscan' or 'agglomerative'.")

    import gc
    import torch

    # After computing embeddings and clustering:
    del model # Delete the loaded SBERT model instance
    del embeddings # Delete the large NumPy array of embeddings

    # Force Python's garbage collector to run
    gc.collect()

    # Clear the GPU cache if a GPU was used (highly recommended)
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    # Validate labels length
    if len(labels) != len(blocks):
        raise RuntimeError(f"Mismatch: len(labels)={len(labels)} but len(blocks)={len(blocks)}")

    # Regrouper
    articles = defaultdict(list)
    for label, b in zip(labels, blocks):
        articles[int(label)].append(b)

    # Sauvegarde
    results = []
    for label, bls in articles.items():
        sorted_blks = sort_blocks_in_cluster(bls)
        article_text = "\n".join([bb.get('text','') for bb in sorted_blks])
        results.append({
            "label": int(label),
            "n_blocks": len(sorted_blks),
            "text": article_text,
            "blocks": sorted_blks
        })
        cluster_name = f"article_{label}" if label != -1 else "noise"
        fname = os.path.join(output_folder, f"{cluster_name}.txt")
        with open(fname, "w", encoding="utf-8") as fo:
            fo.write(article_text)

    # 5. ALIGNEMENT ET ÉVALUATION
    print("[5] Évaluation de la Performance...")

    # a. Charger la Vérité Terrain (GT)
    all_gt_blocks = load_all_ground_truth(xml_folder)

    # b. Alignement des labels prédits (HY) et GT
    # Créez un mapping de l'ID de TextLine au label prédit (HY)
    hy_label_map = {b['id']: int(label) for label, b in zip(labels, blocks_after_merge)}

    # Filtrez et alignez les blocs GT pour ne conserver que ceux qui ont été traités (par ID)
    aligned_gt_labels = []
    aligned_hy_labels = []

    # Utilisez une boucle pour garantir le bon ordre entre GT et HY
    for gt_block in all_gt_blocks:
        tl_id = gt_block['id']
        if tl_id in hy_label_map:
            # GT label (cible)
            aligned_gt_labels.append(gt_block['gt_article_id'])
            # HY label (prédiction)
            aligned_hy_labels.append(hy_label_map[tl_id])

    if not aligned_gt_labels:
        print("!! ERREUR : Aucun bloc aligné pour l'évaluation. Vérifiez les IDs TextLine. !!")
        return None

    # c. Calcul des Métriques
    # Les labels GT sont des chaînes (article_1, article_2, etc.), scikit-learn les gère.
    # Les labels HY sont des entiers (0, 1, 2, ...), scikit-learn les gère.

    ari = adjusted_rand_score(aligned_gt_labels, aligned_hy_labels)
    homogeneity = homogeneity_score(aligned_gt_labels, aligned_hy_labels)
    completeness = completeness_score(aligned_gt_labels, aligned_hy_labels)
    v_measure = v_measure_score(aligned_gt_labels, aligned_hy_labels)

    # d. Affichage des Résultats
    print("\n--- RÉSULTATS D'ÉVALUATION DU CLUSTERING (MÉTRIQUES STANDARDS) ---")
    print(f"Nombre total de TextLines évaluées: {len(aligned_gt_labels)}")
    print(f"Nombre de clusters prédits: {len(np.unique(aligned_hy_labels))}")
    print(f"Nombre de clusters réels (GT): {len(np.unique(aligned_gt_labels))}")
    print("--------------------------------------------------")
    print(f"Adjusted Rand Index (ARI): {ari:.4f} (Mesure globale de similarité de partition)")
    print(f"Homogénéité:               {homogeneity:.4f} (Chaque cluster contient un seul GT article)")
    print(f"Complétude:                {completeness:.4f} (Chaque GT article est dans un seul cluster)")
    print(f"V-Measure (F-Measure) :    {v_measure:.4f} (Score composite équilibré)")
    print("--------------------------------------------------")
    with open(os.path.join(output_folder, "reconstructed_articles.json"), "w", encoding="utf-8") as fo:
        json.dump(results, fo, ensure_ascii=False, indent=2)

    print("[DONE] Résultats dans:", output_folder)
    return aligned_gt_labels

def extract_ground_truth_labels(path):
    """
    Extrait l'ID d'article de la Vérité Terrain (GT) à partir des attributs 'custom'
    des TextLines dans un fichier PAGE-XML.
    """
    tree = etree.parse(str(path))
    root = tree.getroot()
    gt_blocks = []

    for region in root.findall(".//ns:TextRegion", namespaces=NSMAP):
        for tl in region.findall(".//ns:TextLine", namespaces=NSMAP):
            # Chercher l'ID de l'article dans l'attribut 'custom' du TextLine
            custom_attr = tl.get("custom")

            # Le format NewsEye GT est souvent "articleID:article_X"
            article_id = 'NO_ARTICLE'
            if custom_attr and "articleID:" in custom_attr:
                try:
                    # Extrait 'article_X' ou 'article_Y'
                    article_id = custom_attr.split("articleID:")[1].split(';')[0].strip()
                except:
                    # En cas de format inattendu
                    pass

            gt_blocks.append({
                "id": tl.get("id"),
                "source": str(path),
                "gt_article_id": article_id
            })
    return gt_blocks

def load_all_ground_truth(xml_folder_path):
    """ Charge tous les labels GT de toutes les pages. """
    all_gt_blocks = []
    xml_folder = Path(xml_folder_path)
    for f in sorted(xml_folder.glob("*.xml")):
        all_gt_blocks.extend(extract_ground_truth_labels(f))
    return all_gt_blocks

# Call the pipeline function directly with appropriate arguments
xml_folder_path = "/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2" # Replace with your XML folder path
output_folder_path = "/content/chef-douvre/output_sbert" # Replace with your desired output folder path

#Sbert et hdbscan
#pipeline(xml_folder_path, output_folder_path)

#Sbert et agglomorative clustering
pipeline_new(xml_folder_path, output_folder_path)

# from google.colab import files

# si tu veux télécharger un dossier complet
# !zip -r resultats.zip /content/chef-douvre/output_sbert
# files.download("resultats.zip")

from google.colab import files

# si tu veux télécharger un dossier complet
!zip -r resultats.zip /content/chef-douvre/output_sbert
files.download("resultats.zip")
