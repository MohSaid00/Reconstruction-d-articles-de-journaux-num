# -*- coding: utf-8 -*-
"""pipeline(2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I94zRRQVsCE5kg3JBrWlkOosvbXZHRyG
"""

!wget https://huggingface.co/datasets/Jgmorenof/teaching_tools_2025/resolve/main/chef-douvre.zip

# List the files in the zip, take the first 100, and unzip them
!unzip -l chef-douvre.zip 'chef-douvre/AS_TrainingSet_BnF_NewsEye_v2/*' | head -n 101 | tail -n 100 | xargs unzip chef-douvre.zip -d /content/

import os
import glob

# 1. Spécifiez le chemin de votre dossier
# **⚠️ CHANGEZ CETTE LIGNE**
DOSSIER = '/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2'

# 2. Trouvez tous les fichiers JPG/JPEG (ignore la casse)
fichiers_jpg = glob.glob(os.path.join(DOSSIER, '*.[jJ][pP][gG]'))
fichiers_jpeg = glob.glob(os.path.join(DOSSIER, '*.[jJ][pP][eE][gG]'))

tous_les_fichiers = fichiers_jpg + fichiers_jpeg

# 3. Supprimez-les un par un
for fichier in tous_les_fichiers:
    try:
        os.remove(fichier)
        print(f"Supprimé : {fichier}")
    except OSError as e:
        print(f"Erreur lors de la suppression de {fichier} : {e}")

print(f"\nTerminé. {len(tous_les_fichiers)} fichier(s) ciblé(s).")

import os
import json
import argparse
from pathlib import Path
from lxml import etree
from sentence_transformers import SentenceTransformer
import numpy as np
from tqdm import tqdm
from collections import defaultdict
from sklearn.metrics.pairwise import cosine_similarity

# ----------------- Constantes et Fonctions PAGE-XML -----------------
PAGE_NS = "http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15"
NSMAP = {"ns": PAGE_NS}
def get_article_id_from_custom(custom_attr):
    """Extrait l'ID de l'article (e.g., 'a13') de l'attribut custom."""
    if not custom_attr:
        return "N/A"
    # Recherche du motif 'structure {id:XXXX}'
    parts = custom_attr.split('structure {id:')
    if len(parts) > 1:
        # L'ID est après 'structure {id:' et avant la première '}'
        article_id = parts[1].split('}')[0]
        return article_id.strip()
    return "N/A" # Aucune ID d'article trouvée

def coords_from_points(points_str):
    """
    points_str: "x1,y1 x2,y2 x3,y3 ..."
    returns xmin,ymin,xmax,ymax (floats)
    """
    pts = []
    for pair in points_str.strip().split():
        try:
            x_s,y_s = pair.split(",")
            pts.append((float(x_s), float(y_s)))
        except:
            continue
    xs = [p[0] for p in pts] if pts else [0.0]
    ys = [p[1] for p in pts] if pts else [0.0]
    return min(xs), min(ys), max(xs), max(ys)

def parse_page_xml(path):
    """
    Parse PAGE XML file et retourne la liste des blocs (un bloc = un TextRegion)
    avec l'ID d'article et le texte consolidé.
    [MISE À JOUR pour une extraction d'article_id plus robuste]
    """
    # NOTE: path est déjà une chaîne de caractères dans la pipeline,
    # mais on s'assure qu'il est utilisable par etree.parse
    try:
        tree = etree.parse(str(path))
    except Exception as e:
        print(f"Erreur de parsing XML pour {path}: {e}")
        return []

    root = tree.getroot()

    # Dimensions de la page
    page_elem = root.find(".//ns:Page", namespaces=NSMAP)
    image_w = float(page_elem.get("imageWidth")) if page_elem is not None and page_elem.get("imageWidth") else None
    image_h = float(page_elem.get("imageHeight")) if page_elem is not None and page_elem.get("imageHeight") else None
    blocks = []

    # Parcourir TextRegion
    for region in root.findall(".//ns:TextRegion", namespaces=NSMAP):

        # 1. Récupération de l'ID d'Article (Double Vérification pour robustesse)

        # A. Essayer d'abord le custom du TextRegion
        custom_attr_region = region.get("custom")
        article_id = get_article_id_from_custom(custom_attr_region)

        # B. Si l'ID n'a pas été trouvé, essayer le custom du premier TextLine (Fallback)
        if article_id == "N/A":
            first_textline = region.find(".//ns:TextLine", namespaces=NSMAP)
            if first_textline is not None:
                custom_attr_line = first_textline.get("custom")
                article_id = get_article_id_from_custom(custom_attr_line) # Réessaie d'extraire l'ID

        # 2. Récupération du Texte CONSOLIDÉ (Recherche du DERNIER TextEquiv/Unicode dans la Region)
        text = None
        # Chercher tous les TextEquiv/Unicode dans la région. Le dernier est souvent le texte final/consolidé.
        all_unicode_elements = region.findall(".//ns:TextEquiv/ns:Unicode", namespaces=NSMAP)

        if all_unicode_elements:
            # On prend le texte du dernier élément Unicode trouvé dans la région
            text = all_unicode_elements[-1].text.strip() if all_unicode_elements[-1].text else None

        # 3. Fallback Texte: Concaténation des TextLine si aucun texte consolidé n'est trouvé
        if not text:
            textlines = region.findall(".//ns:TextLine", namespaces=NSMAP)
            line_texts = []
            for tl in textlines:
                te = tl.find(".//ns:TextEquiv/ns:Unicode", namespaces=NSMAP)
                # Utiliser le texte de l'Unicode s'il existe, sinon concaténer le contenu textuel direct
                line_text = te.text.strip() if te is not None and te.text else "".join(tl.itertext()).strip()
                if line_text:
                    line_texts.append(line_text)

            text = " ".join(line_texts).strip()

        if not text:
            continue # Ignorer les régions sans texte

        # 4. Récupération des Coordonnées (Bounding Box de la Region)
        coords_node = region.find(".//ns:Coords", namespaces=NSMAP)
        if coords_node is not None and coords_node.get("points"):
            xmin, ymin, xmax, ymax = coords_from_points(coords_node.get("points"))
        else:
            xmin = ymin = 0.0; xmax = 1.0; ymax = 1.0

        # Normalisation
        if image_w and image_h:
            nxmin = xmin / image_w
            nxmax = xmax / image_w
            nymin = ymin / image_h
            nymax = ymax / image_h
        else:
            nxmin, nymin, nxmax, nymax = xmin, ymin, xmax, ymax

        blocks.append({
            "text": text,
            "article_id": article_id,
            "xmin": float(nxmin), "ymin": float(nymin),
            "xmax": float(nxmax), "ymax": float(nymax),
            "image_w": image_w, "image_h": image_h,
            "source": str(path),
            "id": region.get("id") # ID de la TextRegion
        })

    return blocks
# ----------------- Fonctions Utilitaires (Inchangées) -----------------

def save_load_embeddings(texts, embeddings_file_path, model_name='all-MiniLM-L6-v2', batch_size=64, device=None):
    """
    Tente de charger les embeddings à partir du chemin unique fourni.
    Si le fichier n'existe pas ou la taille ne correspond pas, recalcule et sauvegarde.
    """
    embeddings_file = Path(embeddings_file_path)
    non_empty_texts = [t for t in texts if t]

    if not non_empty_texts:
        print("[3] Aucun texte à intégrer (blocs vides).")
        return np.array([])

    try:
        embed_dim = SentenceTransformer(model_name).get_sentence_embedding_dimension()
    except Exception:
        embed_dim = 384
    expected_shape = (len(non_empty_texts), embed_dim)

    if embeddings_file.exists():
        print(f"[3] Chargement des embeddings depuis {embeddings_file}...")
        try:
            embeddings = np.load(embeddings_file)
            if embeddings.shape == expected_shape:
                print("    Chargement réussi et forme correspondante.")
                return embeddings
            print(f"    ATTENTION: Cache invalide. Recalcul...")
        except Exception as e:
            print(f"    Erreur lors du chargement/Vérification: {e}. Recalcul en cours...")

        try:
            os.remove(embeddings_file)
            print(f"    Fichier cache obsolète/corrompu supprimé: {embeddings_file}")
        except:
            pass

    print(f"[3] Calcul et sauvegarde des embeddings SBERT vers {embeddings_file}...")
    embeddings = compute_sbert_embeddings(non_empty_texts, model_name)

    try:
        np.save(embeddings_file, embeddings)
        print(f"    Embeddings sauvegardés avec succès.")
    except Exception as e:
        print(f"    AVERTISSEMENT: Échec de la sauvegarde des embeddings: {e}")

    return embeddings

def load_all_blocks(xml_folder):
    # ... (Votre fonction load_all_blocks inchangée) ...
    xml_folder = Path(xml_folder)
    all_blocks = []
    for f in xml_folder.glob("*.xml"):
        try:
            bls = parse_page_xml(f)
            all_blocks.extend(bls)
        except Exception as e:
            print(f"Erreur parsing {f}: {e}")
    return all_blocks

def merge_short_blocks(blocks, char_threshold=40, y_tol=0.03, x_tol=0.15):
    # ... (Votre fonction merge_short_blocks inchangée) ...
    """
    Blocks coords are normalized [0,1].
    Merge short blocks with a vertical neighbor in the same column.
    y_tol, x_tol are normalized tolerances.
    """
    blocks_by_source = defaultdict(list)
    for b in blocks:
        blocks_by_source[b["source"]].append(b)

    merged = []
    for src, bls in blocks_by_source.items():
        bls_sorted = sorted(bls, key=lambda b: (b["xmin"], b["ymin"]))
        used = [False]*len(bls_sorted)
        for i, b in enumerate(bls_sorted):
            if used[i]: continue

            text = b["text"]

            if len(text) < char_threshold:
                best_j = None
                for j in range(len(bls_sorted)):
                    if i == j or used[j]: continue

                    bj = bls_sorted[j]
                    x_overlap = not (bj["xmax"] < b["xmin"] or bj["xmin"] > b["xmax"])
                    same_col = x_overlap or abs(bj["xmin"] - b["xmin"]) < x_tol
                    vertically_close = abs(b["ymin"] - bj["ymax"]) < y_tol or abs(bj["ymin"] - b["ymax"]) < y_tol

                    if same_col and vertically_close:
                        best_j = j
                        break

                if best_j is not None:
                    bj = bls_sorted[best_j]
                    if b["ymin"] < bj["ymin"]:
                        new_text = b["text"] + " " + bj["text"]
                    else:
                        new_text = bj["text"] + " " + b["text"]

                    bj["text"] = new_text
                    bj["xmin"] = min(bj["xmin"], b["xmin"])
                    bj["ymin"] = min(bj["ymin"], b["ymin"])
                    bj["xmax"] = max(bj["xmax"], b["xmax"])
                    bj["ymax"] = max(bj["ymax"], b["ymax"])

                    used[i] = True
                else:
                    merged.append(b); used[i] = True
            else:
                merged.append(b); used[i] = True

    return merged

def compute_sbert_embeddings(texts, model_name='all-MiniLM-L6-v2', batch_size=64, device=None):
    model = SentenceTransformer(model_name, device=device)
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)
    if embeddings.ndim != 2:
        raise ValueError(f"Embeddings have unexpected dimensions: {embeddings.ndim}")
    if embeddings.shape[0] != len(texts):
         raise ValueError(f"Number of embeddings ({embeddings.shape[0]}) does not match number of texts ({len(texts)})")
    return embeddings

# ----------------- FONCTION DE SIMILARITÉ INTRA-PAGE (NOUVELLE) -----------------

def calculate_and_export_similarity_page_constrained(embeddings, blocks, output_file_path, threshold=0.8):
    """
    Calcule la similarité cosinus, mais uniquement entre les blocs
    provenant du MÊME fichier source (page).
    """
    print(f"[5] Démarrage du calcul de similarité contraint par page (Seuil: {threshold})...")

    non_empty_blocks = [b for b in blocks if b.get('text', '')]

    # Regrouper les indices d'embeddings par fichier source
    embeddings_indices_by_source = defaultdict(list)
    for i, block in enumerate(non_empty_blocks):
        source_name = Path(block["source"]).name
        embeddings_indices_by_source[source_name].append(i)

    high_similarity_pairs = []

    # Créer une matrice de similarité complète une seule fois
    similarity_matrix_full = cosine_similarity(embeddings)

    total_comparisons = 0

    for source_name, indices in tqdm(embeddings_indices_by_source.items(), desc="Comparaison par Page Source"):

        N_page = len(indices)
        if N_page < 2:
            continue

        # Parcourir uniquement les paires à l'intérieur de cette page
        for i_local in range(N_page):
            i_global = indices[i_local] # Index dans la matrice embeddings complète

            for j_local in range(i_local + 1, N_page):
                j_global = indices[j_local] # Index dans la matrice embeddings complète

                # Récupérer le score directement de la matrice complète
                sim = similarity_matrix_full[i_global, j_global]
                total_comparisons += 1

                if sim >= threshold:
                    # Récupérer les données originales des blocs
                    block_i = non_empty_blocks[i_global]
                    block_j = non_empty_blocks[j_global]

                    high_similarity_pairs.append({
                        "similarity": float(sim),
                        "source_page": source_name,
                        "block_1": {
                            "text": block_i["text"][:100] + "..." if len(block_i["text"]) > 100 else block_i["text"],
                            "id": block_i.get("id", "N/A"),
                            "article_id": block_i.get("article_id", "N/A"),
                            "index_global": i_global
                        },
                        "block_2": {
                            "text": block_j["text"][:100] + "..." if len(block_j["text"]) > 100 else block_j["text"],
                            "id": block_j.get("id", "N/A"),
                            "article_id": block_j.get("article_id", "N/A"),
                            "index_global": j_global
                        },
                    })

    print(f"    Total des comparaisons effectuées (intra-page): {total_comparisons}")

    # Sauvegarde des résultats
    with open(output_file_path, "w", encoding="utf-8") as f:
        json.dump(high_similarity_pairs, f, ensure_ascii=False, indent=2)

    print(f"  => Similarités intra-page sauvegardées : {output_file_path} ({len(high_similarity_pairs)} paires trouvées)")
    return high_similarity_pairs

# ----------------- PIPELINE FINAL -----------------

def pipeline_similarity(xml_folder, output_folder,
                        sbert_model='all-MiniLM-L6-v2',
                        embeddings_cache_file="embeddings_cache.npy",
                        merge_short=True,
                        char_threshold=40,
                        similarity_threshold=0.8):
    """
    Pipeline complet : Extraction PAGE-XML (par région), Calcul d'Embeddings,
    et Similarité Cosinus INTRA-PAGE.
    """
    output_path = Path(output_folder)
    os.makedirs(output_path, exist_ok=True)

    print("--- DÉMARRAGE DU PIPELINE SBERT & SIMILARITÉ ---")

    print("[1] Extraction blocs depuis PAGE-XML...")
    blocks = load_all_blocks(xml_folder)
    print(f"  => blocs extraits: {len(blocks)}")

    # ----------------------------------------------------
    # Normalisation des coordonnées (inchangée)
    # ----------------------------------------------------
    max_coord = max(max(b.get('xmax',0), b.get('ymax',0), b.get('xmin',0), b.get('ymin',0)) for b in blocks) if blocks else 1.0
    if max_coord > 1.5:
        print("  -> Normalisation des coordonnées (valeurs absolues détectées)...")
        for b in blocks:
            if b.get('image_w') and b.get('image_h'):
                b['xmin'] /= b['image_w']; b['xmax'] /= b['image_w']
                b['ymin'] /= b['image_h']; b['ymax'] /= b['image_h']
            else:
                b['xmin'] /= max_coord; b['xmax'] /= max_coord
                b['ymin'] /= max_coord; b['ymax'] /= max_coord

    # ----------------------------------------------------
    # Fusion des blocs
    # ----------------------------------------------------
    if merge_short:
        print("[2] Fusion des petits blocs proches...")
        blocks_after_merge = merge_short_blocks(blocks, char_threshold=char_threshold)
        print(f"  => blocs après fusion: {len(blocks_after_merge)}")
    else:
        blocks_after_merge = blocks

    # Filter out empty texts
    texts = [b.get('text', '') for b in blocks_after_merge]

    # ----------------------------------------------------
    # Calcul/Chargement des Embeddings
    # ----------------------------------------------------
    print("[3] Calcul/Chargement embeddings SBERT...")
    embeddings_path = output_path / embeddings_cache_file

    embeddings = save_load_embeddings(
        texts,
        embeddings_path,
        model_name=sbert_model
    )

    if len(embeddings) == 0:
        print("!! AUCUN EMBEDDING à traiter. Fin du pipeline.")
        return

    # ----------------------------------------------------
    # Sauvegarde des Embeddings (Optionnel)
    # ----------------------------------------------------
    export_data = []
    text_idx = 0
    for i, block in enumerate(blocks_after_merge):
        text = block.get('text', '')
        if text:
            export_data.append({
                "index": text_idx,
                "text": text,
                "source_page": Path(block['source']).name,
                "article_id": block.get('article_id', 'N/A'),
                "embedding_vector": embeddings[text_idx].tolist()
            })
            text_idx += 1

    embeddings_json_file = output_path / "blocks_with_embeddings.json"
    with open(embeddings_json_file, "w", encoding="utf-8") as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)
    print(f"  => Données et embeddings exportés : {embeddings_json_file}")

    # ----------------------------------------------------
    # Calcul de la Similarité INTRA-PAGE (Contrainte)
    # ----------------------------------------------------
    similarity_output_file = output_path / f"high_similarity_intra_page_t{str(similarity_threshold).replace('.', '')}.json"

    calculate_and_export_similarity_page_constrained(
        embeddings,
        blocks_after_merge,
        similarity_output_file,
        similarity_threshold
    )

    print("[DONE] Pipeline SBERT & Similarité terminé dans:", output_folder)
    return similarity_output_file

# # Call the pipeline function directly with appropriate arguments
# xml_folder_path = "/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2" # Replace with your XML folder path
# output_folder_path = "/content/chef-douvre/output_sbert" # Replace with your desired output folder path

# #Sbert et hdbscan
# #pipeline(xml_folder_path, output_folder_path)

# #Sbert et agglomorative clustering
# # pipeline_new(xml_folder_path, output_folder_path)

# # from google.colab import files

# # si tu veux télécharger un dossier complet
# # !zip -r resultats.zip /content/chef-douvre/output_sbert
# # files.download("resultats.zip")

# xml_folder_path = "/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2"
# output_folder_path = "/content/chef-douvre/output_sbert"
# embeddings_file = "/content/chef_douvre_embeddings.npy"

# pipeline_new2(
#     xml_folder_path,
#     output_folder_path,
#     embeddings_cache_file=embeddings_file,
#     clustering_kwargs={'n_clusters': 50} # Pass n_clusters within clustering_kwargs
# )

def main():
    parser = argparse.ArgumentParser(description="Extraction, Embedding SBERT et Similarité Cosinus de blocs PAGE-XML.")
    parser.add_argument("xml_folder", type=str, help="Dossier contenant les fichiers PAGE-XML.")
    parser.add_argument("output_folder", type=str, help="Dossier de sortie pour les embeddings et les résultats de similarité.")
    parser.add_argument("--sbert_model", default='all-MiniLM-L6-v2', help="Modèle Sentence-BERT à utiliser.")
    parser.add_argument("--threshold", type=float, default=0.8, help="Seuil de similarité cosinus (0.0 à 1.0).")
    args = parser.parse_args()

    pipeline_similarity(
        args.xml_folder,
        args.output_folder,
        sbert_model=args.sbert_model,
        similarity_threshold=args.threshold
    )

if __name__ == "__main__":
    pipeline_similarity('/content/chef-douvre/AS_TrainingSet_BnF_NewsEye_v2', '/content/output/similarity', similarity_threshold=0.85)
    pass
